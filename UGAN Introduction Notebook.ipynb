{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Representation Learning Using Deep Convolutional Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E4040 Group Project made by:\n",
    "## Team Name UGAN\n",
    "Shambhavi Roy (sr3767)\n",
    "\n",
    "Simran Tiwari (st3400)\n",
    "\n",
    "Saravanan Govindarajan (sg3896)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper by Radford et al.[1] explores the problem of learning image features from large datasets using Generative Adversarial Networks (GANs) instead of deep Convolutional Neural Networks (CNNs). These learned image features can then be used for image classification tasks. This paper also mentions the problem of unstable training of GANs and obtaining nonsensical outputs. To mitigate this, they mention certain architectural constraints for GANs for successful training on large image datasets. This proposed architecture is called Deep Convolutional Generative Adversarial Networks (DCGANs).\n",
    "Further, this paper shows the use of the trained discriminator model of the GAN for image classification and compares the results with other common image classification algorithms. Finally, this paper presents the concept of the learned generator model having vector arithmetic properties that can be used to understand the quality and properties of the generated image samples.\n",
    "\n",
    "\n",
    "Our project is based on this paper. Though this paper mentions the subsequent use of the learned feature representations for image classification tasks and further analysis, our project focuses  on training this architecture on multiple datasets to obtain the learned features and visualizing them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this project is organized in multiple Jupyter notebooks and companion .py files. The root directory also consists of other folders with the saved models, screenshots etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebooks used\n",
    "\n",
    "DCGAN_CIFAR10_NOBN.ipynb is used to train the DCGAN architecture on the CIFAR-10 dataset [2]\n",
    "\n",
    "DCGAN_CelebA_NOBN.ipynb is used to train the DCGAN architecture on the CelebA dataset [3]\n",
    "\n",
    "DCGAN_SVHN_NOBN.ipynb is used to train the DCGAN architecture on the SVHN dataset [4]\n",
    "\n",
    "DCGAN_LSUN_NOBN.ipynb is used to train the DCGAN architecture on the LSUN dataset [10]\n",
    "\n",
    "Guided_Backpropagation_LSUN.ipynb shows the application of Guided Backpropagation to the discriminator trained using the LSUN dataset\n",
    "\n",
    "GradCam_SVHN.ipynb shows the visualization of learned filters using GradCAM [11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Material\n",
    "Project website: https://ecbme4040.github.io/e4040-2021Spring-Project-UGAN-sr3767-st3400-sg3896/\n",
    "\n",
    "Google drive folder: https://drive.google.com/drive/folders/1DQ_by-k33v1k-8cY3zsSSCJC30lUg-8q?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "[1] Radford et al.  “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”, \tarXiv:1511.06434 [Online] https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "[2] CIFAR-10 dataset: Alex Krizhevsky, Learning Multiple Layers of Features from Tiny Images, 2009 [Online] https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf\n",
    "\n",
    "[3] CelebA dataset: Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou, Deep Learning Face Attributes in the Wild, Proceedings of International Conference on Computer Vision (ICCV), December 2015.\n",
    "\n",
    "[4] SVHN dataset: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011.\n",
    "[Online] http://ufldl.stanford.edu/housenumbers/\n",
    "\n",
    "[5] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser and Jianxiong Xiao, LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop, arXiv:1506.03365 [cs.CV], 10 Jun 2015\n",
    "\n",
    "[6] Deep Convolutional Generative Adversarial Network [Online] https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "\n",
    "[7] Having fun with Deep Convolutional GANs[Online] https://naokishibuya.medium.com/having-fun-with-deep-convolutional-gans-f4f8393686ed\n",
    "\n",
    "[8] Deep Convolutional GAN (DCGAN) with SVHN\n",
    "[Online] https://github.com/naokishibuya/deep-learning/blob/master/python/dcgan_svhn.ipynb\n",
    "\n",
    "[9] Goodfellow, NIPS 2016 Tutorial: Generative Adversarial Networks arXiv:1701.00160v4\n",
    "[Online]https://arxiv.org/pdf/1701.00160.pdf\n",
    "\n",
    "[10] 20% sample of LSUN dataset https://www.kaggle.com/jhoward/lsun_bedroom\n",
    "\n",
    "[11] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision, pages 618–626, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -a -I \".git|.ipynb_checkpoints\" >> README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
